{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Cropping Mechanism Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:19:57.623338Z",
     "iopub.status.busy": "2025-11-21T18:19:57.622542Z",
     "iopub.status.idle": "2025-11-21T18:20:00.206872Z",
     "shell.execute_reply": "2025-11-21T18:20:00.206019Z",
     "shell.execute_reply.started": "2025-11-21T18:19:57.623314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths - Ensure these match local environment\n",
    "    RAW_JSON: Path = Path(\"./data/project-4-at-2025-11-19-03-05-8b131c6a.json\")\n",
    "    RAW_IMG_DIR: Path = Path(\"./data/Device_Placement\") \n",
    "    INPUT_SIZE: int = 512\n",
    "    \n",
    "    # Mock parameters\n",
    "    WORK_DIR: Path = Path(\"./output_mock\")\n",
    "    IMG_OUT: Path = WORK_DIR / \"images\"\n",
    "    MASK_OUT: Path = WORK_DIR / \"masks\"\n",
    "    SPLIT_DIR: Path = WORK_DIR / \"splits\"\n",
    "\n",
    "# Main Visualization Class\n",
    "class PipelineVisualizer:\n",
    "    \"\"\"\n",
    "    Visualizes the four stages of the Smart Crop Preprocessing for a single random image.\n",
    "    This class is structurally derived from the original Preprocessor logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        print(\"Initializing YOLO model for subject detection...\")\n",
    "        try:\n",
    "            self.yolo = YOLO('yolo11n-seg.pt')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading YOLO model: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def select_random_image_data(self):\n",
    "        \"\"\"Selects a random image entry and loads its JSON annotation.\"\"\"\n",
    "        IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.bmp', '.tif'}\n",
    "        \n",
    "        try:\n",
    "            with open(self.cfg.RAW_JSON, 'r') as f:  \n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {self.cfg.RAW_JSON}. Cannot proceed.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        valid_items = [x for x in data if x.get('image')]\n",
    "        if not valid_items:\n",
    "            print(\"ERROR: JSON file is empty or missing image entries.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        while True:\n",
    "            # Select a random entry\n",
    "            item = random.choice(valid_items)\n",
    "            \n",
    "            # Replicate the path logic\n",
    "            url = item['image']\n",
    "            if \"?d=\" in url: url = url.split(\"?d=\")[1]\n",
    "            clean_url = urllib.parse.unquote(url).replace(\"\\\\\", \"/\")\n",
    "            fname = Path(clean_url).name\n",
    "            img_path = self.cfg.RAW_IMG_DIR / fname\n",
    "\n",
    "            if img_path.exists() and img_path.suffix.lower() in IMAGE_EXTENSIONS:\n",
    "                print(f\"Selected Random Image: {fname}\")\n",
    "                return img_path, item\n",
    "            \n",
    "            valid_items.remove(item)\n",
    "            if not valid_items:\n",
    "                 print(f\"ERROR: None of the images listed in JSON were found in {self.cfg.RAW_IMG_DIR}\")\n",
    "                 sys.exit(1)\n",
    "\n",
    "\n",
    "    def visualize(self):\n",
    "        img_path, item = self.select_random_image_data()\n",
    "        \n",
    "        # STAGE 1: REAL IMAGE\n",
    "        img_bgr = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img_rgb.shape[:2]\n",
    "        \n",
    "        raw_img = img_rgb.copy()\n",
    "\n",
    "        # STAGE 2: YOLO SEGMENTED IMAGE\n",
    "        results = self.yolo(img_rgb, verbose=False, retina_masks=True)\n",
    "        person_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        \n",
    "        if results[0].masks:\n",
    "            boxes = results[0].boxes\n",
    "            persons = boxes.cls == 0\n",
    "            if persons.any():\n",
    "                idx = torch.argmax(boxes.xywh[persons, 2] * boxes.xywh[persons, 3])\n",
    "                real_idx = persons.nonzero(as_tuple=True)[0][idx]\n",
    "                m = results[0].masks.data[real_idx].cpu().numpy()\n",
    "                if m.shape != (h, w): m = cv2.resize(m, (w, h))\n",
    "                person_mask = (m > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Create a visual overlay for the YOLO segmentation\n",
    "        red_mask = np.zeros_like(img_rgb)\n",
    "        red_mask[:, :, 0] = 255\n",
    "        yolo_segmented_img = np.where(\n",
    "            np.stack((person_mask,) * 3, axis=-1) > 0,\n",
    "            cv2.addWeighted(img_rgb, 0.7, red_mask, 0.3, 0),\n",
    "            img_rgb\n",
    "        )\n",
    "        \n",
    "        # STAGE 3 & 4: CROPPED AND FINAL RESIZE\n",
    "        \n",
    "        # Parse Device Points\n",
    "        device_points = []\n",
    "        if 'label' in item and item['label']:\n",
    "            lbl = item['label'][0]\n",
    "            pts = np.array(lbl['points'])\n",
    "            pts[:, 0] *= (lbl['original_width'] / 100.0)\n",
    "            pts[:, 1] *= (lbl['original_height'] / 100.0)\n",
    "            device_points = pts\n",
    "\n",
    "        # Determine Boundaries\n",
    "        y_head_top = np.argmax(np.any(person_mask, axis=1)) if np.sum(person_mask) > 0 else 0\n",
    "        y_device_bottom = int(np.max(device_points[:, 1])) if len(device_points) > 0 else h\n",
    "\n",
    "        # Calculate Square Dim and Center\n",
    "        roi_height = max(y_device_bottom - y_head_top, h // 3)\n",
    "        square_dim = max(int(roi_height * 1.5), 256)\n",
    "        center_y = y_head_top + (roi_height // 2)\n",
    "        \n",
    "        mask_slice = person_mask[max(0, y_head_top):min(h, y_device_bottom), :]\n",
    "        cols = np.sum(mask_slice, axis=0)\n",
    "        center_x = int(np.dot(np.arange(w), cols) / np.sum(cols)) if np.sum(cols) > 0 else w // 2\n",
    "\n",
    "        # Apply Crop and Padding\n",
    "        half = square_dim // 2\n",
    "        x1, y1 = center_x - half, center_y - half\n",
    "        x2, y2 = x1 + square_dim, y1 + square_dim\n",
    "\n",
    "        # Replicate black background logic\n",
    "        clean_img = np.where(np.stack((person_mask,) * 3, axis=-1) > 0, img_rgb, np.zeros_like(img_rgb))\n",
    "        \n",
    "        pad_l, pad_t = max(0, -x1), max(0, -y1)\n",
    "        pad_r, pad_b = max(0, x2 - w), max(0, y2 - h)\n",
    "\n",
    "        padded_img = cv2.copyMakeBorder(clean_img, pad_t, pad_b, pad_l, pad_r, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "\n",
    "        cx1, cy1 = x1 + pad_l, y1 + pad_t\n",
    "        cropped_img_roi = padded_img[cy1:cy1 + square_dim, cx1:cx1 + square_dim] \n",
    "\n",
    "        # STAGE 4: 512x512 FINAL INPUT\n",
    "        final_input_512 = cv2.resize(cropped_img_roi, (self.cfg.INPUT_SIZE, self.cfg.INPUT_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # VISUALIZATION\n",
    "        self._plot_stages(raw_img, yolo_segmented_img, cropped_img_roi, final_input_512, Path(img_path).name)\n",
    "\n",
    "    def _plot_stages(self, raw, yolo_seg, cropped_roi, final_512, filename):\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.style.use('default')\n",
    "\n",
    "        titles = [\n",
    "            f\"1. Real Image\\n({raw.shape[1]}x{raw.shape[0]})\",\n",
    "            \"2. YOLO Segmented (Subject Isolation)\",\n",
    "            f\"3. Smart Crop ROI\\n({cropped_roi.shape[0]}x{cropped_roi.shape[1]})\",\n",
    "            f\"4. Model Input\\n({final_512.shape[0]}x{final_512.shape[1]} - Normalized)\"\n",
    "        ]\n",
    "        images = [raw, yolo_seg, cropped_roi, final_512]\n",
    "\n",
    "        for i in range(4):\n",
    "            plt.subplot(1, 4, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title(titles[i], fontsize=12, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Smart Crop Pipeline Stages for: {filename}\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Execution Block\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # CRITICAL ACTION: Update these paths to match your local setup.\n",
    "    CFG_VIZ = Config()\n",
    "    \n",
    "    # Demo paths\n",
    "    CFG_VIZ.RAW_IMG_DIR = Path(\"/kaggle/input/patch-placement/Work/InnerGize/Datasets/Device_Placement\") \n",
    "    CFG_VIZ.RAW_JSON = Path(\"/kaggle/input/patch-placement/Work/project-4-at-2025-11-19-03-05-8b131c6a.json\")\n",
    "    \n",
    "    print(\"Starting Smart Crop Visualization Script...\")\n",
    "    \n",
    "    if not (CFG_VIZ.RAW_IMG_DIR.exists() and CFG_VIZ.RAW_JSON.exists()):\n",
    "        print(\"\\n[SETUP ERROR] Please configure the file paths correctly.\")\n",
    "        print(f\"Missing Image Directory: {CFG_VIZ.RAW_IMG_DIR.resolve()}\")\n",
    "        print(f\"Missing JSON File: {CFG_VIZ.RAW_JSON.resolve()}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    viz_pipeline = PipelineVisualizer(CFG_VIZ)\n",
    "    viz_pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Trainning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T10:05:09.790454Z",
     "iopub.status.busy": "2025-11-21T10:05:09.789624Z",
     "iopub.status.idle": "2025-11-21T10:31:11.015639Z",
     "shell.execute_reply": "2025-11-21T10:31:11.015047Z",
     "shell.execute_reply.started": "2025-11-21T10:05:09.790421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "\n",
    "# Dependency Installation\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"SETUP\")\n",
    "\n",
    "logger.info(\"Setting up Environment...\")\n",
    "\n",
    "# Fix OpenCV Conflict\n",
    "os.system('pip uninstall -y opencv-python opencv-contrib-python opencv-python-headless')\n",
    "os.system('pip install -q opencv-python-headless==4.10.0.84') \n",
    "\n",
    "# Install ML Libraries\n",
    "os.system('pip install -q \"numpy<2.0\" \"ultralytics>=8.0.0\" segmentation-models-pytorch albumentations torchmetrics')\n",
    "\n",
    "try:\n",
    "    import segmentation_models_pytorch as smp\n",
    "    from ultralytics import YOLO\n",
    "    import albumentations as albu\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    import torchmetrics\n",
    "except ImportError:\n",
    "    import site\n",
    "    site.main()\n",
    "    import segmentation_models_pytorch as smp\n",
    "    from ultralytics import YOLO\n",
    "    import albumentations as albu\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    import torchmetrics\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration & Logging Setup\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    RAW_JSON: Path = Path(\"/kaggle/input/patch-placement/Work/project-4-at-2025-11-19-03-05-8b131c6a.json\")\n",
    "    RAW_IMG_DIR: Path = Path(\"/kaggle/input/patch-placement/Work/InnerGize/Datasets/Device_Placement\")\n",
    "    WORK_DIR: Path = Path(\"/kaggle/working/prod_pipeline_v1\")\n",
    "    \n",
    "    # Model\n",
    "    ARCH: str = 'UnetPlusPlus'\n",
    "    ENCODER: str = 'resnet34'\n",
    "    INPUT_SIZE: int = 512\n",
    "    \n",
    "    # Training\n",
    "    FOLDS: int = 5\n",
    "    BATCH_SIZE: int = 12\n",
    "    LR: float = 1e-4\n",
    "    EPOCHS: int = 100\n",
    "    PATIENCE: int = 15\n",
    "    DEVICE: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    SEED: int = 42\n",
    "    \n",
    "    # MLOps\n",
    "    EXP_NAME: str = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    \n",
    "    # Derived Paths\n",
    "    IMG_OUT: Path = field(init=False)\n",
    "    MASK_OUT: Path = field(init=False)\n",
    "    MODEL_DIR: Path = field(init=False)\n",
    "    LOG_DIR: Path = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.IMG_OUT = self.WORK_DIR / \"images\"\n",
    "        self.MASK_OUT = self.WORK_DIR / \"masks\"\n",
    "        self.MODEL_DIR = self.WORK_DIR / \"models\"\n",
    "        self.LOG_DIR = self.WORK_DIR / \"logs\" / self.EXP_NAME\n",
    "        \n",
    "        os.makedirs(self.IMG_OUT, exist_ok=True)\n",
    "        os.makedirs(self.MASK_OUT, exist_ok=True)\n",
    "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
    "        os.makedirs(self.LOG_DIR, exist_ok=True)\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "def setup_logger(cfg):\n",
    "    \"\"\"Configures a file logger and a console logger\"\"\"\n",
    "    log_file = cfg.LOG_DIR / \"training.log\"\n",
    "    \n",
    "    root = logging.getLogger()\n",
    "    if root.handlers:\n",
    "        for handler in root.handlers:\n",
    "            root.removeHandler(handler)\n",
    "            \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(\"TRAINER\")\n",
    "\n",
    "LOGGER = setup_logger(CFG)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    LOGGER.info(f\"Seeding with {seed}...\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(CFG.SEED)\n",
    "\n",
    "# Unit Tests\n",
    "def run_unit_tests(cfg):\n",
    "    \"\"\"Runs sanity checks on model and data pipeline before training starts.\"\"\"\n",
    "    LOGGER.info(\"Running Pre-Flight Unit Tests...\")\n",
    "    \n",
    "    # Test 1: Model Architecture\n",
    "    try:\n",
    "        model = smp.UnetPlusPlus(encoder_name=cfg.ENCODER, in_channels=3, classes=1)\n",
    "        dummy_in = torch.randn(2, 3, cfg.INPUT_SIZE, cfg.INPUT_SIZE)\n",
    "        dummy_out = model(dummy_in)\n",
    "        assert dummy_out.shape == (2, 1, cfg.INPUT_SIZE, cfg.INPUT_SIZE)\n",
    "        LOGGER.info(\"    [PASS] Model Architecture Output Shape\")\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f\"    [FAIL] Model Architecture: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Test 2: Augmentation Pipeline\n",
    "    try:\n",
    "        aug = get_transforms('train')\n",
    "        dummy_img = np.random.randint(0, 255, (cfg.INPUT_SIZE, cfg.INPUT_SIZE, 3), dtype=np.uint8)\n",
    "        dummy_mask = np.random.randint(0, 1, (cfg.INPUT_SIZE, cfg.INPUT_SIZE), dtype=np.uint8)\n",
    "        res = aug(image=dummy_img, mask=dummy_mask)\n",
    "        assert res['image'].shape == (3, cfg.INPUT_SIZE, cfg.INPUT_SIZE)\n",
    "        LOGGER.info(\"    [PASS] Augmentation Pipeline\")\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f\"    [FAIL] Augmentations: {e}\")\n",
    "        raise e\n",
    "        \n",
    "    LOGGER.info(\"All Unit Tests Passed. Proceeding to Pipeline.\")\n",
    "\n",
    "# Data Pipeline\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dice = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return 0.5 * self.dice(y_pred, y_true) + 0.5 * self.bce(y_pred, y_true)\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process_data(self):\n",
    "        existing_files = list(self.cfg.IMG_OUT.iterdir())\n",
    "        if len(existing_files) < 5:\n",
    "            LOGGER.info(f\"Starting Smart Crop Pipeline...\")\n",
    "            self._generate_images()\n",
    "        else:\n",
    "            LOGGER.info(f\"Data found ({len(existing_files)} images). Skipping generation.\")\n",
    "        return sorted([f.name for f in self.cfg.IMG_OUT.iterdir()])\n",
    "\n",
    "    def _generate_images(self):\n",
    "        LOGGER.info(\"Loading YOLO for Smart Cropping...\")\n",
    "        yolo = YOLO('yolo11n-seg.pt')\n",
    "        \n",
    "        with open(self.cfg.RAW_JSON, 'r') as f: data = json.load(f)\n",
    "        valid_items = [x for x in data if x.get('image')]\n",
    "        \n",
    "        success_count = 0\n",
    "        for item in tqdm(valid_items, desc=\"Cropping\"):\n",
    "            try:\n",
    "                url = item['image']\n",
    "                if \"?d=\" in url: url = url.split(\"?d=\")[1]\n",
    "                clean_url = urllib.parse.unquote(url).replace(\"\\\\\", \"/\")\n",
    "                fname = Path(clean_url).name\n",
    "                img_path = self.cfg.RAW_IMG_DIR / fname\n",
    "\n",
    "                if not img_path.exists(): continue\n",
    "\n",
    "                img_bgr = cv2.imread(str(img_path))\n",
    "                if img_bgr is None: continue\n",
    "                img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "                h, w = img_rgb.shape[:2]\n",
    "\n",
    "                # Safety Resize\n",
    "                infer_img = img_rgb.copy()\n",
    "                if max(h, w) > 1500:\n",
    "                    scale = 1500 / max(h, w)\n",
    "                    infer_img = cv2.resize(img_rgb, (0,0), fx=scale, fy=scale)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    results = yolo(infer_img, verbose=False, retina_masks=False)\n",
    "                \n",
    "                person_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                if results[0].masks:\n",
    "                    boxes = results[0].boxes\n",
    "                    persons = boxes.cls == 0\n",
    "                    if persons.any():\n",
    "                        idx = torch.argmax(boxes.xywh[persons, 2] * boxes.xywh[persons, 3])\n",
    "                        real_idx = persons.nonzero(as_tuple=True)[0][idx]\n",
    "                        m = results[0].masks.data[real_idx].cpu().numpy()\n",
    "                        m = cv2.resize(m, (w, h))\n",
    "                        person_mask = (m > 0.5).astype(np.uint8)\n",
    "\n",
    "                del results, infer_img\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                gt_mask_full = np.zeros((h, w), dtype=np.uint8)\n",
    "                device_points = []\n",
    "                if 'label' in item and item['label']:\n",
    "                    lbl = item['label'][0]\n",
    "                    pts = np.array(lbl['points'])\n",
    "                    pts[:, 0] *= (lbl['original_width'] / 100.0)\n",
    "                    pts[:, 1] *= (lbl['original_height'] / 100.0)\n",
    "                    device_points = pts\n",
    "                    cv2.fillPoly(gt_mask_full, [pts.astype(np.int32)], 1)\n",
    "\n",
    "                if np.sum(person_mask) > 0:\n",
    "                    rows = np.any(person_mask, axis=1)\n",
    "                    y_head_top = np.argmax(rows)\n",
    "                    cols = np.sum(person_mask, axis=0)\n",
    "                    center_x = int(np.dot(np.arange(w), cols) / (np.sum(cols)+1e-6))\n",
    "                else:\n",
    "                    y_head_top = 0\n",
    "                    center_x = w // 2\n",
    "                \n",
    "                y_device_bottom = int(np.max(device_points[:, 1])) if len(device_points) > 0 else h\n",
    "                roi_height = y_device_bottom - y_head_top\n",
    "                if roi_height < 50: roi_height = h // 3\n",
    "                \n",
    "                square_dim = max(int(roi_height * 1.5), 256)\n",
    "                center_y = y_head_top + (roi_height // 2)\n",
    "\n",
    "                half = square_dim // 2\n",
    "                x1, y1 = center_x - half, center_y - half\n",
    "                x2, y2 = x1 + square_dim, y1 + square_dim\n",
    "\n",
    "                pad_l, pad_t = max(0, -x1), max(0, -y1)\n",
    "                pad_r, pad_b = max(0, x2 - w), max(0, y2 - h)\n",
    "\n",
    "                padded_img = cv2.copyMakeBorder(img_rgb, pad_t, pad_b, pad_l, pad_r, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "                padded_mask = cv2.copyMakeBorder(gt_mask_full, pad_t, pad_b, pad_l, pad_r, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "                cx1, cy1 = x1 + pad_l, y1 + pad_t\n",
    "                cx2, cy2 = cx1 + square_dim, cy1 + square_dim\n",
    "\n",
    "                fin_img = padded_img[cy1:cy2, cx1:cx2]\n",
    "                fin_mask = padded_mask[cy1:cy2, cx1:cx2]\n",
    "\n",
    "                fin_img = cv2.resize(fin_img, (self.cfg.INPUT_SIZE, self.cfg.INPUT_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                fin_mask = cv2.resize(fin_mask, (self.cfg.INPUT_SIZE, self.cfg.INPUT_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                out_name = Path(fname).stem + \".png\"\n",
    "                cv2.imwrite(str(self.cfg.IMG_OUT / out_name), cv2.cvtColor(fin_img, cv2.COLOR_RGB2BGR))\n",
    "                cv2.imwrite(str(self.cfg.MASK_OUT / out_name), fin_mask * 255)\n",
    "                success_count += 1\n",
    "                \n",
    "                if success_count % 10 == 0: gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                LOGGER.warning(f\"Skipped image due to error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        del yolo\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, file_list, cfg, transform=None):\n",
    "        self.files = file_list\n",
    "        self.cfg = cfg\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img_path = str(self.cfg.IMG_OUT / fname)\n",
    "        mask_path = str(self.cfg.MASK_OUT / fname)\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Data Validation\n",
    "        if image is None: raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        if mask is None: raise ValueError(f\"Failed to load mask: {mask_path}\")\n",
    "        if image.shape[:2] != mask.shape[:2]: raise ValueError(f\"Shape mismatch: {fname}\")\n",
    "\n",
    "        if self.transform:\n",
    "            res = self.transform(image=image, mask=mask)\n",
    "            image, mask = res['image'], res['mask']\n",
    "        \n",
    "        return image, mask.unsqueeze(0).float() / 255.0\n",
    "\n",
    "def get_transforms(phase):\n",
    "    base = [albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",
    "    if phase == 'train':\n",
    "        return albu.Compose([\n",
    "            albu.HorizontalFlip(p=0.5),\n",
    "            albu.ShiftScaleRotate(scale_limit=0.15, rotate_limit=15, shift_limit=0.1, border_mode=0, p=0.7),\n",
    "            albu.RandomBrightnessContrast(p=0.4),\n",
    "            albu.HueSaturationValue(p=0.3),\n",
    "            albu.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "        ] + base)\n",
    "    return albu.Compose(base)\n",
    "\n",
    "# Training & Metric Tracking\n",
    "class MetricTracker:\n",
    "    \"\"\"Simulates a remote experiment tracker\"\"\"\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.data = []\n",
    "\n",
    "    def log(self, epoch, train_loss, val_iou, fold):\n",
    "        self.data.append({\n",
    "            \"fold\": fold,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_iou\": val_iou,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.log_dir / \"metrics.json\", \"w\") as f:\n",
    "            json.dump(self.data, f, indent=4)\n",
    "\n",
    "def train_fold(fold_idx, train_files, val_files, cfg, tracker):\n",
    "    LOGGER.info(f\"STARTING FOLD {fold_idx+1}/{cfg.FOLDS} (Train: {len(train_files)}, Val: {len(val_files)})\")\n",
    "\n",
    "    train_ds = SegmentationDataset(train_files, cfg, get_transforms('train'))\n",
    "    val_ds = SegmentationDataset(val_files, cfg, get_transforms('val'))\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = smp.UnetPlusPlus(encoder_name=cfg.ENCODER, encoder_weights=\"imagenet\", in_channels=3, classes=1, activation=None).to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=1e-3)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=1e-6)\n",
    "    criterion = ComboLoss()\n",
    "    metric = torchmetrics.JaccardIndex(task=\"binary\").to(cfg.DEVICE)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_iou = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        model.train()\n",
    "        t_loss = 0\n",
    "        \n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(cfg.DEVICE), masks.to(cfg.DEVICE)\n",
    "            with autocast():\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, masks)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            t_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        metric.reset()\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs, masks = imgs.to(cfg.DEVICE), masks.to(cfg.DEVICE)\n",
    "                with autocast():\n",
    "                    preds = model(imgs)\n",
    "                metric.update(preds.sigmoid() > 0.5, masks.int())\n",
    "        \n",
    "        val_iou = metric.compute().item()\n",
    "        scheduler.step(epoch + val_iou)\n",
    "        \n",
    "        # Track Metrics\n",
    "        t_loss_avg = t_loss/len(train_loader)\n",
    "        tracker.log(epoch, t_loss_avg, val_iou, fold_idx)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            LOGGER.info(f\"    Ep {epoch+1}: Loss {t_loss_avg:.4f} | IoU {val_iou:.4f}\")\n",
    "\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), cfg.MODEL_DIR / f\"model_fold_{fold_idx}.pth\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= cfg.PATIENCE:\n",
    "                LOGGER.info(f\"    Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    del model, optimizer, scaler\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return best_iou\n",
    "\n",
    "# Utils: Export & TTA\n",
    "def export_to_onnx(cfg):\n",
    "    LOGGER.info(\"Exporting Best Model to ONNX...\")\n",
    "    model = smp.UnetPlusPlus(encoder_name=cfg.ENCODER, in_channels=3, classes=1).to(\"cpu\")\n",
    "    weight_path = cfg.MODEL_DIR / \"model_fold_0.pth\"\n",
    "    \n",
    "    if not weight_path.exists():\n",
    "        LOGGER.error(\"Model weights not found for export.\")\n",
    "        return\n",
    "    \n",
    "    model.load_state_dict(torch.load(weight_path, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    \n",
    "    dummy_input = torch.randn(1, 3, cfg.INPUT_SIZE, cfg.INPUT_SIZE)\n",
    "    out_path = cfg.WORK_DIR / \"device_segmentation.onnx\"\n",
    "    \n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model, dummy_input, out_path,\n",
    "            input_names=[\"input\"], output_names=[\"output\"],\n",
    "            opset_version=11,\n",
    "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        LOGGER.info(f\"Export successful: {out_path}\")\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f\"Export failed: {e}\")\n",
    "\n",
    "def predict_with_tta(model, image_tensor):\n",
    "    \"\"\"Test Time Augmentation\"\"\"\n",
    "    with torch.no_grad():\n",
    "        p1 = model(image_tensor).sigmoid()\n",
    "        p2 = torch.flip(model(torch.flip(image_tensor, [3])).sigmoid(), [3])\n",
    "    return (p1 + p2) / 2.0\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. Run Unit Tests\n",
    "        run_unit_tests(CFG)\n",
    "        \n",
    "        # 2. Preprocess\n",
    "        prep = Preprocessor(CFG)\n",
    "        all_files = np.array(prep.process_data())\n",
    "        \n",
    "        # 3. Setup Experiment Tracker\n",
    "        tracker = MetricTracker(CFG.LOG_DIR)\n",
    "        \n",
    "        # 4. Cross Validation\n",
    "        kf = KFold(n_splits=CFG.FOLDS, shuffle=True, random_state=CFG.SEED)\n",
    "        fold_scores = []\n",
    "        \n",
    "        LOGGER.info(f\"Starting Training: {CFG.FOLDS} Folds | {CFG.ARCH} | {CFG.ENCODER}\")\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(all_files)):\n",
    "            score = train_fold(fold, all_files[train_idx], all_files[val_idx], CFG, tracker)\n",
    "            fold_scores.append(score)\n",
    "            LOGGER.info(f\"Fold {fold+1} Result: {score:.4f}\")\n",
    "            \n",
    "        # 5. Save Metrics\n",
    "        tracker.save()\n",
    "        LOGGER.info(f\"AVERAGE IoU: {np.mean(fold_scores):.4f}\")\n",
    "        LOGGER.info(f\"Metrics saved to {CFG.LOG_DIR}/metrics.json\")\n",
    "        \n",
    "        # 6. Export\n",
    "        export_to_onnx(CFG)\n",
    "        \n",
    "        # 7. Visual Validation\n",
    "        LOGGER.info(\"Generating Visual Report...\")\n",
    "        model = smp.UnetPlusPlus(encoder_name=CFG.ENCODER, in_channels=3, classes=1).to(CFG.DEVICE)\n",
    "        model.load_state_dict(torch.load(CFG.MODEL_DIR / \"model_fold_0.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        _, val_idx = next(kf.split(all_files))\n",
    "        val_ds = SegmentationDataset(all_files[val_idx], CFG, get_transforms('val'))\n",
    "        val_loader = DataLoader(val_ds, batch_size=3, shuffle=True)\n",
    "        \n",
    "        imgs, masks = next(iter(val_loader))\n",
    "        imgs = imgs.to(CFG.DEVICE)\n",
    "        preds = predict_with_tta(model, imgs)\n",
    "        \n",
    "        # Plot\n",
    "        imgs = imgs.cpu().numpy()\n",
    "        masks = masks.cpu().numpy()\n",
    "        preds = preds.cpu().numpy()\n",
    "        \n",
    "        mean = np.array([0.485, 0.456, 0.406]).reshape(3,1,1)\n",
    "        std = np.array([0.229, 0.224, 0.225]).reshape(3,1,1)\n",
    "        \n",
    "        fig, axes = plt.subplots(len(imgs), 3, figsize=(10, 3*len(imgs)))\n",
    "        if len(imgs) == 1: axes = np.array([axes])\n",
    "        \n",
    "        for i in range(len(imgs)):\n",
    "            viz_img = np.clip((imgs[i] * std + mean).transpose(1,2,0), 0, 1)\n",
    "            axes[i,0].imshow(viz_img); axes[i,0].set_title(\"Input\")\n",
    "            axes[i,1].imshow(masks[i].squeeze(), cmap='gray'); axes[i,1].set_title(\"Truth\")\n",
    "            axes[i,2].imshow(preds[i].squeeze() > 0.5, cmap='jet'); axes[i,2].set_title(\"Pred (TTA)\")\n",
    "            for ax in axes[i]: ax.axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        LOGGER.info(\"Pipeline Completed Successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        LOGGER.critical(f\"Critical Pipeline Failure: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to use the model for placement prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:10:37.814962Z",
     "iopub.status.busy": "2025-11-21T11:10:37.814319Z",
     "iopub.status.idle": "2025-11-21T11:10:43.685607Z",
     "shell.execute_reply": "2025-11-21T11:10:43.684715Z",
     "shell.execute_reply.started": "2025-11-21T11:10:37.814933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import urllib.parse\n",
    "\n",
    "# Install Inference Dependencies\n",
    "os.system('pip install -q onnxruntime-gpu')\n",
    "\n",
    "class PatchPredictor:\n",
    "    def __init__(self, onnx_model_path, yolo_model_path='yolo11n-seg.pt'):\n",
    "        print(f\"Loading ONNX Model: {onnx_model_path}...\")\n",
    "        \n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        try:\n",
    "            self.session = ort.InferenceSession(str(onnx_model_path), providers=providers)\n",
    "        except Exception:\n",
    "            print(\"CUDA not found. Falling back to CPU.\")\n",
    "            self.session = ort.InferenceSession(str(onnx_model_path), providers=['CPUExecutionProvider'])\n",
    "            \n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.img_size = 512\n",
    "        \n",
    "        print(\"Loading YOLO for Smart Cropping...\")\n",
    "        self.yolo = YOLO(yolo_model_path)\n",
    "\n",
    "    def preprocess(self, image_path):\n",
    "        img_bgr = cv2.imread(str(image_path))\n",
    "        if img_bgr is None: raise ValueError(f\"Image not found: {image_path}\")\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img_rgb.shape[:2]\n",
    "\n",
    "        # Smart Crop\n",
    "        infer_img = img_rgb.copy()\n",
    "        if max(h, w) > 1500:\n",
    "            scale = 1500 / max(h, w)\n",
    "            infer_img = cv2.resize(img_rgb, (0,0), fx=scale, fy=scale)\n",
    "        \n",
    "        results = self.yolo(infer_img, verbose=False, retina_masks=False)\n",
    "        \n",
    "        person_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        if results[0].masks:\n",
    "            boxes = results[0].boxes\n",
    "            persons = boxes.cls == 0\n",
    "            if persons.any():\n",
    "                idx = results[0].boxes.conf[persons].argmax()\n",
    "                real_idx = persons.nonzero(as_tuple=True)[0][idx]\n",
    "                m = results[0].masks.data[real_idx].cpu().numpy()\n",
    "                m = cv2.resize(m, (w, h))\n",
    "                person_mask = (m > 0.5).astype(np.uint8)\n",
    "\n",
    "        if np.sum(person_mask) > 0:\n",
    "            rows = np.any(person_mask, axis=1)\n",
    "            y_head_top = np.argmax(rows)\n",
    "            cols = np.sum(person_mask, axis=0)\n",
    "            center_x = int(np.dot(np.arange(w), cols) / (np.sum(cols)+1e-6))\n",
    "        else:\n",
    "            y_head_top = 0\n",
    "            center_x = w // 2\n",
    "        \n",
    "        roi_height = h // 3\n",
    "        square_dim = max(int(roi_height * 1.5), 256)\n",
    "        center_y = y_head_top + (roi_height // 2)\n",
    "\n",
    "        half = square_dim // 2\n",
    "        x1, y1 = center_x - half, center_y - half\n",
    "        x2, y2 = x1 + square_dim, y1 + square_dim\n",
    "\n",
    "        pad_l, pad_t = max(0, -x1), max(0, -y1)\n",
    "        pad_r, pad_b = max(0, x2 - w), max(0, y2 - h)\n",
    "\n",
    "        padded_img = cv2.copyMakeBorder(img_rgb, pad_t, pad_b, pad_l, pad_r, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "        \n",
    "        cx1, cy1 = x1 + pad_l, y1 + pad_t\n",
    "        cx2, cy2 = cx1 + square_dim, cy1 + square_dim\n",
    "\n",
    "        crop_img = padded_img[cy1:cy2, cx1:cx2]\n",
    "        input_img = cv2.resize(crop_img, (self.img_size, self.img_size), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Normalization\n",
    "        norm_img = input_img.astype(np.float32) / 255.0\n",
    "        norm_img = (norm_img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        blob = np.transpose(norm_img, (2, 0, 1))\n",
    "        blob = np.expand_dims(blob, axis=0)\n",
    "        \n",
    "        meta = {\n",
    "            'padded_img': padded_img,\n",
    "            'crop_coords': (cx1, cy1, cx2, cy2),\n",
    "            'pad_info': (pad_t, pad_l, h, w)\n",
    "        }\n",
    "        \n",
    "        return blob.astype(np.float32), meta\n",
    "\n",
    "    def predict_raw(self, input_tensor):\n",
    "        outputs = self.session.run(None, {self.input_name: input_tensor})\n",
    "        logits = outputs[0][0, 0, :, :]\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "        return probs\n",
    "\n",
    "    def visualize(self, image_path):\n",
    "        try:\n",
    "            input_tensor, meta = self.preprocess(image_path)\n",
    "            \n",
    "            # Get Probabilities\n",
    "            probs = self.predict_raw(input_tensor)\n",
    "            max_conf = np.max(probs)\n",
    "            print(f\"    Max Conf: {max_conf:.4f}\")\n",
    "\n",
    "            # Adaptive Thresholding\n",
    "            thresholds = [0.5, 0.3, 0.15]\n",
    "            mask = None\n",
    "            final_thresh = 0.5\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                temp_mask = (probs > thresh).astype(np.uint8)\n",
    "                if np.sum(temp_mask) > 50: # Ensure at least 50 pixels are detected\n",
    "                    mask = temp_mask\n",
    "                    final_thresh = thresh\n",
    "                    print(f\"    Patch detected at threshold: {thresh}\")\n",
    "                    break\n",
    "            \n",
    "            padded_img = meta['padded_img']\n",
    "            cx1, cy1, cx2, cy2 = meta['crop_coords']\n",
    "            pad_t, pad_l, h, w = meta['pad_info']\n",
    "            \n",
    "            overlay = padded_img.copy()\n",
    "\n",
    "            if mask is not None:\n",
    "                # Resize mask to crop size\n",
    "                crop_h, crop_w = cy2 - cy1, cx2 - cx1\n",
    "                real_scale_mask = cv2.resize(mask, (crop_w, crop_h), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Place in full image\n",
    "                full_mask = np.zeros((padded_img.shape[0], padded_img.shape[1]), dtype=np.uint8)\n",
    "                full_mask[cy1:cy2, cx1:cx2] = real_scale_mask\n",
    "                \n",
    "                # Draw Green Overlay\n",
    "                color_mask = np.zeros_like(padded_img)\n",
    "                color_mask[:, :] = [0, 255, 0] \n",
    "                \n",
    "                overlay = np.where(full_mask[:, :, None] == 1, \n",
    "                                cv2.addWeighted(padded_img, 0.7, color_mask, 0.3, 0), \n",
    "                                padded_img)\n",
    "                \n",
    "                # Draw White Contour\n",
    "                contours, _ = cv2.findContours(full_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                cv2.drawContours(overlay, contours, -1, (255, 255, 255), 3)\n",
    "            else:\n",
    "                print(\"    No patch detected.\")\n",
    "                # Draw Text on image indicating failure\n",
    "                cv2.putText(overlay, f\"No Patch Detected (Max Conf: {max_conf:.2f})\", \n",
    "                        (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "\n",
    "            # Remove padding\n",
    "            final_view = overlay[pad_t:pad_t+h, pad_l:pad_l+w]\n",
    "\n",
    "            plt.figure(figsize=(12, 12))\n",
    "            plt.imshow(final_view)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"File: {image_path.name} | Conf: {max_conf:.2f}\", fontsize=14)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Skipping {image_path.name}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ONNX_PATH = Path(\"/kaggle/working/prod_pipeline_v1/device_segmentation.onnx\")\n",
    "    \n",
    "    # Update this path to your target directory\n",
    "    RAW_DIR = Path(\"/kaggle/input/sample3\")\n",
    "    \n",
    "    if ONNX_PATH.exists():\n",
    "        predictor = PatchPredictor(ONNX_PATH)\n",
    "        \n",
    "        # Gather all images\n",
    "        all_images = sorted(list(RAW_DIR.glob(\"*.jpg\")) + list(RAW_DIR.glob(\"*.png\")) + list(RAW_DIR.glob(\"*.jpeg\")))\n",
    "        \n",
    "        if all_images:\n",
    "            print(f\"Found {len(all_images)} images in directory. Processing all...\")\n",
    "            for idx, test_img in enumerate(all_images):\n",
    "                print(f\"\\n[{idx+1}/{len(all_images)}] Processing: {test_img.name}\")\n",
    "                predictor.visualize(test_img)\n",
    "        else:\n",
    "            print(\"No images found to test.\")\n",
    "    else:\n",
    "        print(\"Please run training first.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8793337,
     "sourceId": 13809442,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8797788,
     "sourceId": 13815858,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8797873,
     "sourceId": 13815967,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8797924,
     "sourceId": 13816031,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8797943,
     "sourceId": 13816060,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
